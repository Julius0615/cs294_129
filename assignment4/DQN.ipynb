{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this notebook, you will implement a deep Q-Learning reinforcement algorithm. The implementation borrows ideas from both the original DeepMind Nature paper and the more recent asynchronous version:<br/>\n",
    "[1] \"Human-Level Control through Deep Reinforcement Learning\" by Mnih et al. 2015<br/>\n",
    "[2] \"Asynchronous Methods for Deep Reinforcement Learning\" by Mnih et al. 2016.<br/>\n",
    "\n",
    "In particular:\n",
    "* We use separate target and Q-functions estimators with periodic updates to the target estimator. \n",
    "* We use several concurrent \"threads\" rather than experience replay to generate less biased gradient updates. \n",
    "* Threads are actually synchronized so we start each one at a random number of moves.\n",
    "* We use an epsilon-greedy policy that blends random moves with policy moves.\n",
    "* We taper the random action parameter (epsilon) and the learning rate to zero during training.\n",
    "\n",
    "This gives a simple and reasonably fast general-purpose RL algorithm. We use it here for the Cartpole environment from OpenAI Gym, but it can easily be adapted to others. For this notebook, you will implement 4 steps:\n",
    "\n",
    "1. The backward step for the Q-estimator\n",
    "2. The $\\epsilon$-greedy policy\n",
    "3. \"asynchronous\" initialization \n",
    "4. The Q-learning algorithm\n",
    "\n",
    "To get started, we import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=:0\n"
     ]
    }
   ],
   "source": [
    "%env DISPLAY :0\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below lists some parameters you can tune. They should be self-explanatory. They are currently set to train CartPole-V0 to a \"solved\" score (> 195) most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsteps = 80001                       # Number of steps to run (game actions per environment)\n",
    "npar = 16                            # Number of parallel environments\n",
    "target_window = 200                  # Interval to update target estimator from q-estimator\n",
    "discount_factor = 0.99               # Reward discount factor\n",
    "printsteps = 1000                    # Number of steps between printouts\n",
    "render = False                       # Whether to render an environment while training\n",
    "\n",
    "epsilon_start = 1.0                  # Parameters for epsilon-greedy policy: initial epsilon\n",
    "epsilon_end = 0.0                    # Final epsilon\n",
    "neps = int(0.8*nsteps)               # Number of steps to decay epsilon\n",
    "\n",
    "learning_rate = 2e-3                 # Initial learning rate\n",
    "lr_end = 0                           # Final learning rate\n",
    "nlr = neps                           # Steps to decay learning rate\n",
    "decay_rate = 0.99                    # Decay factor for RMSProp \n",
    "\n",
    "nhidden = 200                        # Number of hidden layers for estimators\n",
    "\n",
    "init_moves = 2000                    # Upper bound on random number of moves to take initially\n",
    "nwindow = 2                          # Sensing window = last n images in a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are environment-specific parameters. The function \"preprocess\" should process an observation returned by the environment into a vector for training. For CartPole we simply append a 1 to implement bias in the first layer. \n",
    "\n",
    "For visual environments you would typically crop, downsample to 80x80, set color to a single bit (foreground/background), and flatten to a vector. That transformation is already implemented in the Policy Gradient code.\n",
    "\n",
    "*nfeats* is the dimension of the vector output by *preprocess*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "game_type=\"CartPole-v0\"                 # Model type and action definitions\n",
    "VALID_ACTIONS = [0, 1]\n",
    "nactions = len(VALID_ACTIONS)\n",
    "nfeats = 5                              # There are four state features plus the constant we add\n",
    "\n",
    "def preprocess(I):                      # preprocess each observation\n",
    "    \"\"\"Just append a 1 to the end\"\"\"\n",
    "    return np.append(I.astype(float),1) # Add a constant feature for bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Q-estimator class. We use two instances of this class, one for the target estimator, and one for the Q-estimator. The Q function is normally represented as a scalar $Q(x,a)$ where $x$ is the state and $a$ is an action. For ease of implementation, we actually estimate a vector-valued function $Q(x,.)$ which returns the estimated reward for every action. The model here has just a single hidden layer:\n",
    "\n",
    "<pre>\n",
    "Input Layer (nfeats) => FC Layer => RELU => FC Layer => Output (naction values)\n",
    "</pre>\n",
    "\n",
    "## 1. Implement Q-estimator gradient\n",
    "Your first task is to implement the\n",
    "<pre>Estimator.gradient(s, a, y)</pre>\n",
    "method for this class. **gradient** should compute the gradients wrt weight arrays W1 and W2 into\n",
    "<pre>self.grad['W1']\n",
    "self.grad['W2']</pre>\n",
    "respectively. Both <code>a</code> and <code>y</code> are vectors. Be sure to update only the output layer weights corresponding to the given action vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "\n",
    "    def __init__(self, ninputs, nhidden, nactions):\n",
    "        \"\"\" Create model matrices, and gradient and squared gradient buffers\"\"\"\n",
    "        model = {}\n",
    "        model['W1'] = np.random.randn(nhidden, ninputs) / np.sqrt(ninputs)   # \"Xavier\" initialization\n",
    "        model['W2'] = np.random.randn(nactions, nhidden) / np.sqrt(nhidden)\n",
    "        self.model = model\n",
    "        self.grad = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "        self.gradsq = { k : np.zeros_like(v) for k,v in model.iteritems() }\n",
    "        \n",
    "\n",
    "    def forward(self, s):\n",
    "        \"\"\" Run the model forward given a state as input.\n",
    "        returns action predictions and the hidden state\"\"\"\n",
    "        h = np.dot(self.model['W1'], s)\n",
    "        h[h<0] = 0 # ReLU nonlinearity\n",
    "        rew = np.dot(self.model['W2'], h)\n",
    "        return rew, h\n",
    "    \n",
    "    \n",
    "    def predict(self, s):\n",
    "        \"\"\" Predict the action rewards from a given input state\"\"\"\n",
    "        rew, h = self.forward(s)\n",
    "        return rew\n",
    "    \n",
    "              \n",
    "    def gradient(self, s, a, y):\n",
    "        \"\"\" Given a state s, action a and target y, compute the model gradients\"\"\"\n",
    "        ##################################################################################\n",
    "        ##                                                                              ##\n",
    "        ## TODO: Compute gradients and return a scalar loss on a minibatch of size npar ##\n",
    "        ##    s is the input state matrix (ninputs x npar).                             ##\n",
    "        ##    a is an action vector (npar,).                                            ##\n",
    "        ##    y is a vector of target values (npar,) corresponding to those actions.    ##\n",
    "        ##    return: the loss per sample (npar,).                                      ##\n",
    "        ##                                                                              ##\n",
    "        ## Notes:                                                                       ##\n",
    "        ##    * If the action is ai in [0,...,nactions-1], backprop only through the    ##\n",
    "        ##      ai'th output.                                                           ##\n",
    "        ##    * loss should be L2, and we recommend you normalize it to a per-input     ##\n",
    "        ##      value, i.e. return L2(target,predition)/sqrt(npar).                     ##\n",
    "        ##    * save the gradients in self.grad['W1'] and self.grad['W2'].              ##\n",
    "        ##    * update self.grad['W1'] and self.grad['W2'] by adding the gradients, so  ##\n",
    "        ##      that multiple gradient steps can be used beteween updates.              ##\n",
    "        ##                                                                              ##\n",
    "        ##################################################################################\n",
    "        N = s.shape[0]\n",
    "        y_, h = self.forward(s)\n",
    "        loss = (y_[a, xrange(npar)] - y) ** 2 / np.sqrt(npar)\n",
    "        d_out = np.zeros_like(y_)  # n_actions x npar\n",
    "        d_out[a, xrange(npar)] = 0.5 * (y_[a, xrange(npar)] - y) / np.sqrt(npar)\n",
    "        self.grad['W2'] += -np.dot(d_out, h.T)\n",
    "        d_h = np.dot(self.model['W2'].T, d_out)\n",
    "        d_h[h <= 0] = 0\n",
    "        self.grad['W1'] += -np.dot(d_h, s.T)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def rmsprop(self, learning_rate, decay_rate): \n",
    "        \"\"\" Perform model updates from the gradients using RMSprop\"\"\"\n",
    "        for k in self.model:\n",
    "            g = self.grad[k]\n",
    "            self.gradsq[k] = decay_rate * self.gradsq[k] + (1 - decay_rate) * g*g\n",
    "            self.model[k] += learning_rate * g / (np.sqrt(self.gradsq[k]) + 1e-5)\n",
    "            self.grad[k].fill(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement $\\epsilon$-Greedy Policy\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take a uniformly-random action.\n",
    "* otherwise choose the best action according to the estimator from the given state.\n",
    "\n",
    "The function below should implement this policy. It should return a matrix A of size (nactions, npar) such that A[i,j] is the probability of taking action i on input j. The probabilities of non-optimal actions should be $\\epsilon/{\\rm nactions}$ and the probability of the best action should be $1-\\epsilon+\\epsilon/{\\rm nactions}$.\n",
    "\n",
    "Since the function processes batches of states, the input <code>state</code> is a <code>ninputs x npar</code> matrix, and the returned value should be a <code>nactions x npar</code> matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy(estimator, state, epsilon):\n",
    "    \"\"\" Take an estimator and state and predict the best action.\n",
    "    For each input state, return a vector of action probabilities according to an epsilon-greedy policy\"\"\"\n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Implement an epsilon-greedy policy                                     ##\n",
    "    ##       estimator: is the estimator to use (instance of Estimator)             ##\n",
    "    ##       state is an (ninputs x npar) state matrix                              ##\n",
    "    ##       epsilon is the scalar policy parameter                                 ##\n",
    "    ## return: an (nactions x npar) matrix A where A[i,j] is the probability of     ##\n",
    "    ##       taking action i on input j.                                            ##\n",
    "    ##                                                                              ##\n",
    "    ## Use the definition of epsilon-greedy from the cell above.                    ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "\n",
    "    A = np.ones((nactions, npar)) * epsilon / nactions\n",
    "    rew = estimator.predict(state)\n",
    "    actions = np.argmax(rew, axis=0)\n",
    "    A[actions, xrange(npar)] += 1 - epsilon\n",
    "    A = A / np.sum(A, axis=0).reshape(1, -1)\n",
    "    return A\n",
    "\n",
    "def sample_actions(action_probs):\n",
    "    actions = []\n",
    "    for i in xrange(npar):\n",
    "        try:\n",
    "            actions.append(np.random.choice(nactions, p=action_probs[:, i]))\n",
    "        except ValueError:\n",
    "            import pdb; pdb.set_trace()\n",
    "    return np.array(actions, dtype=int)\n",
    "\n",
    "def expected_rewards(estimator, state, epsilon):\n",
    "    A = policy(estimator, state, epsilon)\n",
    "    rewards = estimator.predict(state)\n",
    "    return np.sum(A * rewards, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine copies the state of one estimator into another. Its used to update the target estimator from the Q-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_estimator(to_estimator, from_estimator, window, istep):\n",
    "    \"\"\" every <window> steps, Copy model state from from_estimator into to_estimator\"\"\"\n",
    "    if (istep % window == 0):\n",
    "        for k in from_estimator.model:\n",
    "            np.copyto(to_estimator.model[k], from_estimator.model[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement \"Asynchronous Threads\"\n",
    "\n",
    "Don't try that in Python!! Actually all we do here is create an array of environments and advance each one a random number of steps, using random actions at each step. Later on we will make *synchronous* updates to all the environments, but the environments (and their gradient updates) should remain uncorrelated. This serves the same goal as asynchronous updates in paper [2], or experience replay in paper [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-22 01:52:45,433] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,450] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,500] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,541] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,585] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,616] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,636] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,644] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,680] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,706] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,718] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,729] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,754] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,785] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,830] Making new env: CartPole-v0\n",
      "[2016-11-22 01:52:45,835] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "block_reward = 0.0;\n",
    "total_epochs = 0;\n",
    "   \n",
    "# Create estimators\n",
    "q_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "target_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "\n",
    "# The epsilon and learning rate decay schedules\n",
    "epsilons = np.linspace(epsilon_start, epsilon_end, neps)\n",
    "learning_rates = np.linspace(learning_rate, lr_end, nlr)\n",
    "\n",
    "# Initialize the games\n",
    "print(\"Initializing games...\"); sys.stdout.flush()\n",
    "envs = np.empty(npar, dtype=object)\n",
    "state = np.zeros([nfeats * nwindow, npar], dtype=float)\n",
    "rewards = np.zeros([npar], dtype=float)\n",
    "dones = np.empty(npar, dtype=int)\n",
    "actions = np.zeros([npar], dtype=int)\n",
    "\n",
    "\n",
    "def append_state(all_states, obs, env_idx):\n",
    "    all_states = all_states.copy()\n",
    "    env_states = all_states[:, env_idx].reshape(nfeats, nwindow)\n",
    "    env_states[:, :nwindow - 1] = env_states[:, 1:]\n",
    "    env_states[:, nwindow - 1] = obs\n",
    "    all_states[:, env_idx] = env_states.flatten()\n",
    "    return all_states\n",
    "\n",
    "def reset_state(all_states, env_idx):\n",
    "    all_states = all_states.copy()\n",
    "    all_states[:, env_idx] = 0\n",
    "    return all_states\n",
    "\n",
    "\n",
    "for i in range(npar):\n",
    "    envs[i] = gym.make(game_type)\n",
    "    for j in xrange(np.random.randint(nwindow, init_moves + 1)):\n",
    "        act = np.random.randint(0, nactions)\n",
    "        actions[i] = act\n",
    "        obs, reward, done, info = envs[i].step(act)\n",
    "        obs = preprocess(obs)\n",
    "        rewards[i] = reward\n",
    "        state = append_state(state, obs, i)\n",
    "        if done:\n",
    "            obs = envs[i].reset()\n",
    "            obs = preprocess(obs)\n",
    "            state = reset_state(state, i)\n",
    "            state = append_state(state, obs, i)\n",
    "            total_epochs += 1\n",
    "            rewards[i] = 0\n",
    "    dones[i] = False\n",
    "            \n",
    "        \n",
    "   \n",
    "    ##################################################################################\n",
    "    ##                                                                              ##\n",
    "    ## TODO: Advance each environment by a random number of steps, where the number ##\n",
    "    ##       of steps is sampled uniformly from [nwindow, init_moves].              ##\n",
    "    ##       Use random steps to advance.                                           ## \n",
    "    ##                                                                              ##\n",
    "    ## Update the total reward and total epochs variables as you go.                ##\n",
    "    ## If an environment returns done=True, reset it and increment the epoch count. ##\n",
    "    ##                                                                              ##\n",
    "    ##################################################################################\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Deep Q-Learning\n",
    "In this cell you actually implement the algorithm. We've given you comments to define all the steps. You should also add book-keeping steps to keep track of the loss, reward and number of epochs (where env.step() returns done = true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, time 0.0, loss 0.00152804, epochs 34606, reward/epoch 0.00046\n",
      "step 1000, time 0.9, loss 0.00180882, epochs 35326, reward/epoch 0.02222\n",
      "step 2000, time 1.7, loss 0.00253071, epochs 36017, reward/epoch 0.02315\n",
      "step 3000, time 2.6, loss 0.00162605, epochs 36716, reward/epoch 0.02289\n",
      "step 4000, time 3.4, loss 0.00287959, epochs 37408, reward/epoch 0.02312\n",
      "step 5000, time 4.3, loss 0.00440850, epochs 38083, reward/epoch 0.02370\n",
      "step 6000, time 5.2, loss 0.00221407, epochs 38719, reward/epoch 0.02516\n",
      "step 7000, time 6.0, loss 0.00237455, epochs 39321, reward/epoch 0.02658\n",
      "step 8000, time 6.9, loss 0.00274701, epochs 39965, reward/epoch 0.02484\n",
      "step 9000, time 7.7, loss 0.00281877, epochs 40611, reward/epoch 0.02477\n",
      "step 10000, time 8.6, loss 0.00246463, epochs 41293, reward/epoch 0.02346\n",
      "step 11000, time 9.5, loss 0.00359251, epochs 41931, reward/epoch 0.02508\n",
      "step 12000, time 10.3, loss 0.00371448, epochs 42592, reward/epoch 0.02421\n",
      "step 13000, time 11.2, loss 0.00247078, epochs 43171, reward/epoch 0.02763\n",
      "step 14000, time 12.0, loss 0.00208969, epochs 43714, reward/epoch 0.02947\n",
      "step 15000, time 12.9, loss 0.00288311, epochs 44384, reward/epoch 0.02388\n",
      "step 16000, time 13.7, loss 0.00250591, epochs 44942, reward/epoch 0.02867\n",
      "step 17000, time 14.6, loss 0.00270525, epochs 45556, reward/epoch 0.02606\n",
      "step 18000, time 15.4, loss 0.00280005, epochs 46154, reward/epoch 0.02676\n",
      "step 19000, time 16.3, loss 0.00284908, epochs 46744, reward/epoch 0.02712\n",
      "step 20000, time 17.2, loss 0.00198202, epochs 47410, reward/epoch 0.02402\n",
      "step 21000, time 18.0, loss 0.00360921, epochs 48112, reward/epoch 0.02279\n",
      "step 22000, time 18.9, loss 0.00249506, epochs 48775, reward/epoch 0.02413\n",
      "step 23000, time 19.7, loss 0.00293912, epochs 49383, reward/epoch 0.02632\n",
      "step 24000, time 20.6, loss 0.00299221, epochs 49915, reward/epoch 0.03008\n",
      "step 25000, time 21.4, loss 0.00292256, epochs 50448, reward/epoch 0.03002\n",
      "step 26000, time 22.3, loss 0.00290302, epochs 50892, reward/epoch 0.03604\n",
      "step 27000, time 23.1, loss 0.00268166, epochs 51328, reward/epoch 0.03670\n",
      "step 28000, time 24.0, loss 0.00317964, epochs 51933, reward/epoch 0.02645\n",
      "step 29000, time 24.8, loss 0.00301915, epochs 52299, reward/epoch 0.04372\n",
      "step 30000, time 25.7, loss 0.00222171, epochs 52818, reward/epoch 0.03083\n",
      "step 31000, time 26.6, loss 0.00272011, epochs 53457, reward/epoch 0.02504\n",
      "step 32000, time 27.4, loss 0.00259628, epochs 54034, reward/epoch 0.02773\n",
      "step 33000, time 28.3, loss 0.00232802, epochs 54424, reward/epoch 0.04103\n",
      "step 34000, time 29.1, loss 0.00221334, epochs 54706, reward/epoch 0.05674\n",
      "step 35000, time 30.0, loss 0.00219821, epochs 55111, reward/epoch 0.03951\n",
      "step 36000, time 30.8, loss 0.00312195, epochs 55726, reward/epoch 0.02602\n",
      "step 37000, time 31.7, loss 0.00226876, epochs 56205, reward/epoch 0.03340\n",
      "step 38000, time 32.6, loss 0.00227393, epochs 56459, reward/epoch 0.06299\n",
      "step 39000, time 33.4, loss 0.00257546, epochs 56817, reward/epoch 0.04469\n",
      "step 40000, time 34.3, loss 0.00230832, epochs 57206, reward/epoch 0.04113\n",
      "step 41000, time 35.1, loss 0.00245497, epochs 57591, reward/epoch 0.04156\n",
      "step 42000, time 36.0, loss 0.00251972, epochs 57960, reward/epoch 0.04336\n",
      "step 43000, time 36.8, loss 0.00183401, epochs 58161, reward/epoch 0.07960\n",
      "step 44000, time 37.7, loss 0.00121181, epochs 58464, reward/epoch 0.05281\n",
      "step 45000, time 38.6, loss 0.00352943, epochs 58856, reward/epoch 0.04082\n",
      "step 46000, time 39.4, loss 0.00220995, epochs 59282, reward/epoch 0.03756\n",
      "step 47000, time 40.3, loss 0.00152197, epochs 59499, reward/epoch 0.07373\n",
      "step 48000, time 41.1, loss 0.00188171, epochs 59701, reward/epoch 0.07921\n",
      "step 49000, time 42.0, loss 0.00158742, epochs 60111, reward/epoch 0.03902\n",
      "step 50000, time 42.8, loss 0.00196359, epochs 60352, reward/epoch 0.06639\n",
      "step 51000, time 43.7, loss 0.00210506, epochs 60454, reward/epoch 0.15686\n",
      "step 52000, time 44.6, loss 6.28992684, epochs 60581, reward/epoch 0.12598\n",
      "step 53000, time 45.4, loss 0.00156982, epochs 60747, reward/epoch 0.09639\n",
      "step 54000, time 46.3, loss 0.00141148, epochs 60934, reward/epoch 0.08556\n",
      "step 55000, time 47.1, loss 0.00142058, epochs 61162, reward/epoch 0.07018\n",
      "step 56000, time 48.0, loss 0.00165406, epochs 61366, reward/epoch 0.07843\n",
      "step 57000, time 48.8, loss 0.00158424, epochs 61504, reward/epoch 0.11594\n",
      "step 58000, time 49.7, loss 0.00262731, epochs 61740, reward/epoch 0.06780\n",
      "step 59000, time 50.5, loss 0.00142174, epochs 62088, reward/epoch 0.04598\n",
      "step 60000, time 51.4, loss 0.00168332, epochs 62252, reward/epoch 0.09756\n",
      "step 61000, time 52.2, loss 0.00139806, epochs 62489, reward/epoch 0.06751\n",
      "step 62000, time 53.1, loss 0.00140836, epochs 62613, reward/epoch 0.12903\n",
      "step 63000, time 53.9, loss 0.00137567, epochs 62831, reward/epoch 0.07339\n",
      "step 64000, time 54.8, loss 0.00718057, epochs 62995, reward/epoch 0.09756\n",
      "step 65000, time 55.7, loss 0.00120792, epochs 63118, reward/epoch 0.13008\n",
      "step 66000, time 56.5, loss 0.00112669, epochs 63585, reward/epoch 0.03426\n",
      "step 67000, time 57.4, loss 0.00558910, epochs 63750, reward/epoch 0.09697\n",
      "step 68000, time 58.2, loss 0.00223715, epochs 63991, reward/epoch 0.06639\n",
      "step 69000, time 59.1, loss 0.00125277, epochs 64231, reward/epoch 0.06667\n",
      "step 70000, time 59.9, loss 0.00131873, epochs 64459, reward/epoch 0.07018\n",
      "step 71000, time 60.8, loss 0.00146853, epochs 64943, reward/epoch 0.03306\n",
      "step 72000, time 61.6, loss 0.01310438, epochs 65204, reward/epoch 0.06130\n",
      "step 73000, time 62.5, loss 0.00138761, epochs 65371, reward/epoch 0.09581\n",
      "step 74000, time 63.4, loss 0.00248810, epochs 65572, reward/epoch 0.07960\n",
      "step 75000, time 64.2, loss 0.00125349, epochs 65777, reward/epoch 0.07805\n",
      "step 76000, time 65.1, loss 0.00126040, epochs 65951, reward/epoch 0.09195\n",
      "step 77000, time 65.9, loss 0.00197446, epochs 66239, reward/epoch 0.05556\n",
      "step 78000, time 66.8, loss 0.00185156, epochs 66563, reward/epoch 0.04938\n",
      "step 79000, time 67.6, loss 0.00120530, epochs 66880, reward/epoch 0.05047\n",
      "step 80000, time 68.5, loss 0.00279054, epochs 67154, reward/epoch 0.05839\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "block_loss = 0.0\n",
    "last_epochs=0\n",
    "\n",
    "for istep in np.arange(nsteps): \n",
    "    if (render): envs[0].render()\n",
    "  \n",
    "    #########################################################################\n",
    "    ## TODO: Implement Q-Learning                                          ##\n",
    "    ##                                                                     ##\n",
    "    ## At high level, your code should:                                    ##\n",
    "    ## * Update epsilon and learning rate.                                 ##\n",
    "    ## * Update target estimator from Q-estimator if needed.               ##\n",
    "    ## * Get the next action probabilities for the minibatch by running    ##\n",
    "    ##   the policy on the current state with the Q-estimator.             ##\n",
    "    ## * Then for each environment:                                        ##\n",
    "    ##     ** Pick an action according to the action probabilities.        ##\n",
    "    ##     ** Step in the gym with that action.                            ##\n",
    "    ##     ** Process the observation and concat it to the last nwindow-1  ##\n",
    "    ##        processed observations to form a new state.                  ##\n",
    "    ## Then for all environments (vectorized):                             ##\n",
    "    ## * Predict Q-scores for the new state using the target estimator.    ##\n",
    "    ## * Compute new expected rewards using those Q-scores.                ##\n",
    "    ## * Using those expected rewards as a target, compute gradients and   ##\n",
    "    ##   update the Q-estimator.                                           ##\n",
    "    ## * Step to the new state.                                            ##\n",
    "    ##                                                                     ##\n",
    "    #########################################################################\n",
    "    current_eps = epsilons[int(1.0 * istep / nsteps * neps)]\n",
    "    current_lr = learning_rates[int(1.0 * istep / nsteps * nlr)]\n",
    "\n",
    "    update_estimator(target_estimator, q_estimator, target_window, istep)\n",
    "\n",
    "    old_state = state.copy()\n",
    "\n",
    "    action_probs = policy(q_estimator, state, current_eps)\n",
    "    actions = sample_actions(action_probs)\n",
    "\n",
    "    for i in range(npar):\n",
    "        if dones[i]:\n",
    "            obs = envs[i].reset()\n",
    "            obs = preprocess(obs)\n",
    "            state = reset_state(state, i)\n",
    "            state = append_state(state, obs, i)\n",
    "            total_epochs += 1\n",
    "            rewards[i] = 0\n",
    "\n",
    "        obs, reward, done, info = envs[i].step(actions[i])\n",
    "        dones[i] = done\n",
    "\n",
    "        rewards[i] = reward\n",
    "\n",
    "        obs = preprocess(obs)\n",
    "        state = append_state(state, obs, i)\n",
    "\n",
    "#     import pdb; pdb.set_trace()\n",
    "    expected_new_rewards = expected_rewards(target_estimator, state, current_eps)\n",
    "    expected_new_rewards = expected_new_rewards * np.logical_not(done)\n",
    "    target_q = expected_new_rewards * discount_factor + rewards\n",
    "    loss = q_estimator.gradient(old_state, actions, target_q)\n",
    "    block_loss = np.sum(loss)\n",
    "    q_estimator.rmsprop(current_lr, decay_rate)\n",
    "    block_reward = np.sum(rewards)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "   \n",
    "    t = time.time() - t0\n",
    "    if (istep % printsteps == 0): \n",
    "        print(\"step {:0d}, time {:.1f}, loss {:.8f}, epochs {:0d}, reward/epoch {:.5f}\".format(\n",
    "                istep, t, block_loss/printsteps, total_epochs, block_reward/np.maximum(1,total_epochs-last_epochs)))\n",
    "        last_epochs = total_epochs\n",
    "        block_reward = 0.0\n",
    "        block_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(q_estimator.model, open(\"cartpole_q_estimator.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reload the model later if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_estimator = Estimator(nfeats*nwindow, nhidden, nactions)\n",
    "test_estimator.model = pickle.load(open(\"cartpole_q_estimator.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And animate the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n",
      "reset\n"
     ]
    }
   ],
   "source": [
    "state0 = state[:,0]\n",
    "for i in np.arange(200):\n",
    "    envs[0].render()\n",
    "    preds = test_estimator.predict(state0)\n",
    "    iaction = np.argmax(preds)\n",
    "    obs, _, done0, _ = envs[0].step(VALID_ACTIONS[iaction])\n",
    "    state0 = np.concatenate((state0[nfeats:], preprocess(obs)))\n",
    "    if (done0): envs[0].reset(); print \"reset\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there we have it. Simple 1-step Q-Learning can solve easy problems very fast. Note that environments that produce images will be much slower to train on than environments (like CartPole) which return an observation of the state of the system. But this model can still train on those image-based games - like Atari games. It will take hours-days however. It you try training on visual environments, we recommend you run the most expensive step - rmsprop - less often (e.g. every 10 iterations). This gives about a 3x speedup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional\n",
    "Do **one** of the following tasks:\n",
    "* Adapt the DQN algorithm to another environment - it can use direct state observations.  Call <code>env.get_action_meanings()</code> to find out what actions are allowed. Summarize training performance: your final average reward/epoch, the number of steps required to train, and any modifications to the model or its parameters that you made.\n",
    "* Try smarter schedules for epsilon and learning rate. Rewards for CartPole increase very sharply (several orders of magnitude) with better policies, especially as epsilon --> 0. Gradients will also change drastically, so the initial learning rate is probably not good later on. Try schedules for decreasing epsilon that allow the model to better adapt. Try other learning rate schedules, or setting learning rate based on average reward. \n",
    "* Try a fancier model. e.g. add another hidden layer, or try sigmoid non-linearities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
